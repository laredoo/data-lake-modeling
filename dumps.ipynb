{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/lucaslaredo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/lucaslaredo/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ee58c6bd-9643-48a5-9012-830bd3287ab8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 442ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ee58c6bd-9643-48a5-9012-830bd3287ab8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/17ms)\n",
      "25/02/03 22:22:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/03 22:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/03 22:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/02/03 22:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/02/03 22:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/02/03 22:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/02/03 22:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/02/03 22:22:19 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spotify-datalake\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024M\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlist paths\n",
    "playlists_v1_path = '/shared/sampled/playlists_v1.json'\n",
    "playlists_v2_path = '/shared/sampled/playlists_v2.json'\n",
    "playlists_v3_path = '/shared/sampled/playlists_v3.json'\n",
    "\n",
    "# tracks paths\n",
    "tracks_v1_path = '/shared/sampled/tracks_v1.json'\n",
    "tracks_v2_path = '/shared/sampled/tracks_v2.json'\n",
    "tracks_v3_path = '/shared/sampled/tracks_v3.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "playlists_v1_df = spark.read.json(playlists_v1_path)\n",
    "tracks_v1_df = spark.read.json(tracks_v1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collaborative', 'description', 'name', 'pid']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists_v1_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['album_name',\n",
       " 'album_uri',\n",
       " 'artist_name',\n",
       " 'artist_uri',\n",
       " 'duration_ms',\n",
       " 'pid',\n",
       " 'pos',\n",
       " 'track_name',\n",
       " 'track_uri']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_v1_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "song_information = tracks_v1_df.select(\n",
    "    \"track_name\",\n",
    "    \"track_uri\",\n",
    "    \"duration_ms\",\n",
    "    \"album_uri\",\n",
    "    \"artist_uri\"\n",
    ").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "album_information = tracks_v1_df.select(\n",
    "    \"album_name\",\n",
    "    \"album_uri\",\n",
    "    \"artist_uri\"\n",
    ").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_information = tracks_v1_df.select(\n",
    "    \"artist_name\",\n",
    "    \"artist_uri\"\n",
    ").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_information = playlists_v1_df.select(\n",
    "    \"name\",\n",
    "    \"pid\",\n",
    "    \"description\",\n",
    "    \"collaborative\"\n",
    ").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_tracks_information = tracks_v1_df.select(\n",
    "    \"pid\",\n",
    "    \"pos\",\n",
    "    \"track_uri\",\n",
    "    \"album_uri\",\n",
    "    \"artist_uri\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/03 23:30:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/03 23:30:49 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# save all to datalake\n",
    "import time\n",
    "\n",
    "# PARQUET\n",
    "parquet_start_time = time.time()\n",
    "song_information.write.mode(\"overwrite\").parquet(\"./silver/parquet/songs/\")\n",
    "album_information.write.mode(\"overwrite\").parquet(\"./silver/parquet/album_information/\")\n",
    "artists_information.write.mode(\"overwrite\").parquet(\"./silver/parquet/artists_information/\")\n",
    "playlists_information.write.mode(\"overwrite\").parquet(\"./silver/parquet/playlists/\")\n",
    "playlist_tracks_information.write.mode(\"overwrite\").parquet(\"./silver/parquet/playlist_tracks/\")\n",
    "silver_write_parquet_time = time.time() - parquet_start_time\n",
    "\n",
    "# JSON\n",
    "json_start_time = time.time()\n",
    "song_information.write.mode(\"overwrite\").json(\"./silver/json/songs/\")\n",
    "album_information.write.mode(\"overwrite\").json(\"./silver/json/album_information/\")\n",
    "artists_information.write.mode(\"overwrite\").json(\"./silver/json/artists_information/\")\n",
    "playlists_information.write.mode(\"overwrite\").json(\"./silver/json/playlists/\")\n",
    "playlist_tracks_information.write.mode(\"overwrite\").json(\"./silver/json/playlist_tracks/\")\n",
    "silver_write_json_time = time.time() - json_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track_name', 'track_uri', 'duration_ms', 'album_uri', 'artist_uri']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_information.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['album_name', 'album_uri', 'artist_uri']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "album_information.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist_name', 'artist_uri']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists_information.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'pid', 'description', 'collaborative']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists_information.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pid', 'pos', 'track_uri', 'album_uri', 'artist_uri']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlist_tracks_information.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "read_parquet_start_time = time.time()\n",
    "silver_playlist_tracks = spark.read.parquet(\"./silver/parquet/playlist_tracks/\")\n",
    "silver_playlists = spark.read.parquet(\"./silver/parquet/playlists/\")\n",
    "silver_songs = spark.read.parquet(\"./silver/parquet/songs/\")\n",
    "silver_artists = spark.read.parquet(\"./silver/parquet/artists_information/\")\n",
    "silver_album = spark.read.parquet(\"./silver/parquet/album_information/\")\n",
    "silver_read_parquet_time = time.time() - read_parquet_start_time\n",
    "\n",
    "read_json_start_time = time.time()\n",
    "json_silver_playlist_tracks = spark.read.json(\"./silver/json/playlist_tracks/\")\n",
    "json_silver_playlists = spark.read.json(\"./silver/json/playlists/\")\n",
    "json_silver_songs = spark.read.json(\"./silver/json/songs/\")\n",
    "json_silver_artists = spark.read.json(\"./silver/json/artists_information/\")\n",
    "json_silver_album = spark.read.json(\"./silver/json/album_information/\")\n",
    "json_silver_read_parquet_time = time.time() - read_json_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track_name', 'track_uri', 'duration_ms', 'album_uri', 'artist_uri']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_songs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'pid', 'description', 'collaborative']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_playlists.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pid', 'pos', 'track_uri', 'album_uri', 'artist_uri']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_playlist_tracks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+-----------+\n",
      "|duration_ms|     song_artist_uri|           album_uri|           track_uri|playlist_id|\n",
      "+-----------+--------------------+--------------------+--------------------+-----------+\n",
      "|     229120|spotify:artist:1R...|spotify:album:3sY...|spotify:track:004...|     116299|\n",
      "|     164346|spotify:artist:57...|spotify:album:5K2...|spotify:track:005...|      38723|\n",
      "|     164346|spotify:artist:57...|spotify:album:5K2...|spotify:track:005...|     151838|\n",
      "|     273613|spotify:artist:6H...|spotify:album:3oJ...|spotify:track:005...|     109979|\n",
      "|     273613|spotify:artist:6H...|spotify:album:3oJ...|spotify:track:005...|      18046|\n",
      "+-----------+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pre_gold_playlist_info.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, countDistinct, col\n",
    "\n",
    "pre_gold_playlist_info = silver_playlist_tracks.alias(\"spt\").join(\n",
    "    silver_songs.alias(\"ss\"), col(\"spt.track_uri\") == col(\"ss.track_uri\"), \"inner\"\n",
    ").select(\n",
    "    col(\"ss.duration_ms\"),\n",
    "    col(\"ss.artist_uri\").alias(\"song_artist_uri\"),  # Renomeia para evitar ambiguidade\n",
    "    col(\"ss.album_uri\"),\n",
    "    col(\"ss.track_uri\"),\n",
    "    col(\"spt.pid\").alias(\"playlist_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_playlist_info = pre_gold_playlist_info.groupBy(\"playlist_id\").agg(\n",
    "    col(\"playlist_id\").alias(\"pid\"),\n",
    "    spark_sum(\"duration_ms\").alias(\"total_duration_ms\"),\n",
    "    countDistinct(\"track_uri\").alias(\"num_tracks\"),\n",
    "    countDistinct(\"song_artist_uri\").alias(\"num_artists\"),\n",
    "    countDistinct(\"album_uri\").alias(\"num_albums\")\n",
    ").join(silver_playlists, \"pid\", \"inner\").select(\n",
    "    col(\"pid\").alias(\"playlist_id\"),\n",
    "    \"total_duration_ms\",\n",
    "    \"num_tracks\",\n",
    "    \"num_artists\",\n",
    "    \"num_albums\",\n",
    "    \"name\",\n",
    "    \"description\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:10:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 115:>                                                        (0 + 8) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------+-----------+----------+---------------+-----------+\n",
      "|playlist_id|total_duration_ms|num_tracks|num_artists|num_albums|           name|description|\n",
      "+-----------+-----------------+----------+-----------+----------+---------------+-----------+\n",
      "|      11276|         11314441|        51|         17|        25|   Stuff I Like|       null|\n",
      "|      13460|          3221562|        15|          4|         8|     21 Savage |       null|\n",
      "|      37884|          8341215|        18|          8|         9|           Mood|       null|\n",
      "|     105182|          1202078|         5|          5|         5|     fave songs|       null|\n",
      "|     116139|          9364728|        38|         28|        31|This is for you|       null|\n",
      "+-----------+-----------------+----------+-----------+----------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "gold_playlist_info.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist_name', 'artist_uri']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_artists.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_playlist_tracks_v1 = silver_playlist_tracks.alias(\"spt\").join(\n",
    "    silver_songs.alias(\"ss\"), col(\"spt.track_uri\") == col(\"ss.track_uri\"), \"inner\"\n",
    ").select(\n",
    "    col(\"ss.artist_uri\"),\n",
    "    col(\"ss.album_uri\"),\n",
    "    col(\"ss.track_name\"),\n",
    "    col(\"spt.pos\"),\n",
    "    col(\"spt.pid\").alias(\"playlist_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 144:==============>                                          (2 + 6) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+-----------+\n",
      "|          artist_uri|           album_uri|          track_name|pos|playlist_id|\n",
      "+--------------------+--------------------+--------------------+---+-----------+\n",
      "|spotify:artist:1R...|spotify:album:3sY...|            Magnolia| 12|     116299|\n",
      "|spotify:artist:57...|spotify:album:5K2...|I Hear A Symphony...|  5|      38723|\n",
      "|spotify:artist:57...|spotify:album:5K2...|I Hear A Symphony...| 27|     151838|\n",
      "|spotify:artist:6H...|spotify:album:3oJ...|     Fuck Everything|  9|     109979|\n",
      "|spotify:artist:6H...|spotify:album:3oJ...|     Fuck Everything| 22|      18046|\n",
      "+--------------------+--------------------+--------------------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "gold_playlist_tracks_v1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_playlist_tracks = gold_playlist_tracks_v1.join(\n",
    "    silver_artists, \"artist_uri\", \"inner\"\n",
    ").select(\n",
    "    \"playlist_id\",\n",
    "    \"pos\",\n",
    "    \"track_name\",\n",
    "    \"album_uri\",\n",
    "    \"artist_name\"\n",
    ").join(\n",
    "    silver_album, \"album_uri\", \"inner\"\n",
    ").select(\n",
    "    \"playlist_id\",\n",
    "    \"pos\",\n",
    "    \"track_name\",\n",
    "    \"artist_name\",\n",
    "    \"album_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+--------------------+---------------+--------------------+\n",
      "|playlist_id|pos|          track_name|    artist_name|          album_name|\n",
      "+-----------+---+--------------------+---------------+--------------------+\n",
      "|     116299| 12|            Magnolia| The Hush Sound|          Like Vines|\n",
      "|      38723|  5|I Hear A Symphony...|   The Supremes|I Hear A Symphony...|\n",
      "|     151838| 27|I Hear A Symphony...|   The Supremes|I Hear A Symphony...|\n",
      "|     109979|  9|     Fuck Everything|Suicide Silence|     The Black Crown|\n",
      "|      18046| 22|     Fuck Everything|Suicide Silence|     The Black Crown|\n",
      "+-----------+---+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_playlist_tracks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/03 23:23:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# saving gold layer\n",
    "\n",
    "gold_playlist_info.write.mode(\"overwrite\").parquet(\"./gold/playlists/\")\n",
    "gold_playlist_tracks.write.mode(\"overwrite\").parquet(\"./gold/playlist_tracks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/datalake/gold/playlist_info_json.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# JSON Load\u001b[39;00m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m df_json \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/datalake/gold/playlist_info_json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m json_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Parquet Load\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py:418\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/datalake/gold/playlist_info_json."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# JSON Load\n",
    "start_time = time.time()\n",
    "df_json = spark.read.json(\"./gold/playlist_info_json\")\n",
    "json_time = time.time() - start_time\n",
    "\n",
    "# Parquet Load\n",
    "start_time = time.time()\n",
    "df_parquet = spark.read.parquet(\"/datalake/gold/playlist_info_parquet\")\n",
    "parquet_time = time.time() - start_time\n",
    "\n",
    "print(f\"JSON Load Time: {json_time:.2f} seconds\")\n",
    "print(f\"Parquet Load Time: {parquet_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
